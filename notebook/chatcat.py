# -*- coding: utf-8 -*-
"""chatcat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q2F-n1trlaG8jHASfrl90jVOJ1BYlWwK
"""



# If running in Google Colab, you may need to run this cell to make sure you're using UTF-8 locale to install LangChain
import locale
locale.getpreferredencoding = lambda: "UTF-8"

!pip install -qU torch transformers accelerate bitsandbytes transformers sentence-transformers faiss-gpu pypdf langchain_community langchain

!pip install fastapi uvicorn

import os
import torch
from pathlib import Path

from typing import List
from fastapi import FastAPI

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from google.colab import drive
drive.mount('/content/drive')
pdf_dir = Path('/content/drive/MyDrive/data')

documents = []

for filename in os.listdir(pdf_dir):
    if filename.endswith('.pdf'):
        pdf_path = os.path.join(pdf_dir, filename)

        # Load the PDF document
        pdf_loader = PyPDFLoader(pdf_path)
        loaded_docs = pdf_loader.load()

        documents.extend(loaded_docs)
        print(f"Loaded {filename} with {len(loaded_docs)} documents")

splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)

chunked_docs = splitter.split_documents(documents)
print(f"Split into {len(chunked_docs)} chunks")
# Check if chunked_docs is empty and handle the error
if not chunked_docs:
    print("Error: Chunked documents are empty. Check your document loading and splitting process.")
    # Handle the error, e.g., by exiting or skipping this step
    # exit()  # or continue to the next step in your workflow
else:
    # Create a vector database if chunked_docs is not empty
    db = FAISS.from_documents(chunked_docs, HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5"))

    # Create a retriever to search the vector store
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 4})

model_name = "HuggingFaceH4/zephyr-7b-beta"

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Create a text generation pipeline using the HuggingFace pipeline
text_generation_pipeline = pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    temperature=0.2,
    do_sample=True,
    repetition_penalty=1.1,
    return_full_text=True,
    max_new_tokens=400,
)

llm = HuggingFacePipeline(pipeline=text_generation_pipeline)

# Create a prompt template to generate the prompt for the LLM
prompt_template = """
<|system|>
Answer the question based on your knowledge. Use the following context to help:

{context}

</s>
<|user|>
{question}
</s>
<|assistant|>

 """

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)

# Create a chain with the prompt and the LLM
llm_chain = prompt | llm | StrOutputParser()

# Create a chain with the retriever and the LLM chain

rag_chain = {"context": retriever, "question": RunnablePassthrough()} | llm_chain

question = "What is the Software Architecture and Design course about?"

llm_chain.invoke({"context": "", "question": question})

rag_chain.invoke(question)

!pip install uvicorn pydantic

!pip install pyngrok fastapi uvicorn nest_asyncio --upgrade # Upgrade to the latest version of nest_asyncio

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import nest_asyncio
from pyngrok import ngrok
from langchain import PromptTemplate, LLMChain

# Initialize FastAPI app
app = FastAPI()

# Add CORS middleware
# origins = ["http://localhost", "http://localhost:8080", "http://localhost:4200"]  # Update with your Angular app's origin
origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Define a Pydantic model for the request body
class Query(BaseModel):
    query_text: str


# Define the API endpoint

@app.get("/")
def read_root():
    return {"Hello": "World"}

@app.post("/query/")
async def query(query: Query):
    query_text = query.query_text  # Access query_text from the model

    # Retrieve relevant documents
    docs = retriever.get_relevant_documents(query_text)

    # Define the prompt template
    template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know.

    {context}

    Question: {question}
    """
    prompt = PromptTemplate(
        template=template,
        input_variables=["context", "question"],
    )
    # Create the chain
    chain = LLMChain(llm=llm, prompt=prompt)  # Assuming 'llm' is your language model
    # Invoke the chain and get the response
    response = await chain.arun(context=docs, question=query_text)
    return {"answer": response}

# Start the FastAPI server
if __name__ == "__main__":
    import nest_asyncio
    nest_asyncio.apply() # This line is added to patch the event loop for compatibility with Jupyter Notebooks

    # Start the server on a specific port (e.g., 8090)
    server = uvicorn.Server(config=uvicorn.Config(app, host="127.0.0.1", port=8090))

    async def start_server_and_ngrok():
        await server.serve()  # Start the FastAPI server

    # Create an ngrok tunnel
    ngrok.set_auth_token("2nPjpHvs9rYEDdNq9q3OBf3pp8q_5agvmB5JiEvrbrLrgg3Mw")
    public_url = ngrok.connect(8090).public_url
    print(f"Your FastAPI app is now accessible at: {public_url}")

  # Run the server using uvicorn's run function
    # uvicorn.run(app, host="127.0.0.1", port=8090)
    import asyncio
    loop = asyncio.get_event_loop()
    loop.run_until_complete(start_server_and_ngrok())
    # OR
    # asyncio.run(start_server_and_ngrok()) # If you're using Python 3.7+